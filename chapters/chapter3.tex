\chapter{Applied Statistic Methods}
\label{definition_correlation}
This chapter will explain the later used statical methods and their mathematical definitions. This is to provide an in depth understanding how the results, presented in later chapters are comprised. This also clarify which methods and assumptions implemented to handle a mixed data-based analysis.

\bigskip

Correlation is an analysis procedure that measures the correlation coefficient, which represents the degree of linear, bivariant, monotonic or other kind of relation, which could also be described as the degree of association between two variables \parencite{HerzSchlicherSiegener1992}. In most statistics the following common types of can be found: Pearson's $r$, Kendall's $\tau$, Spearman  $\rho$ or the Point-Biserial correlation \parencite{Ramzai2020,SPSS2020a,SPSS2020b}. Besides off these, there are many more correlation coefficients which varying in they applicability and interpretability. Depending on which type of data variables are to be analyzed, it it necessary to choose an applicable correlation coefficient. The type of data variable and relation combination are the most restricting features for choosing a suitable correlation coefficient. 

\section{Variable Types}
\label{correlation_variable_types}
Data variables can be grouped into continuous and categorical, depending what kind of observation they describe. Variables are considered to be continuous, also know as quantitative, when relating to measurements like speed, distance or age, which can take on an unlimited number of values between the lowest and highest points of measurement \parencite{McCue2007}. These continuous variables can be separated into two subsets. 

\begin{itemize}
	\item \textbf{Interval} variables can be measured along a continuum and have a numerical value. \parencite{Laerd2020}
    \item \textbf{Ratio} variables are interval variables, with the added condition that the values is zero if there is no measurement for this value. \parencite{Laerd2020}
\end{itemize}

Categorical variables on the other hand are limited in the number of values, referring to a category, rank or choice, like a vehicle type or Yes/No answers. These categorical variables can be separated into three subsets.

\begin{itemize}
	\item \textbf{Nominal} variables have two or more categories, but with no intrinsic order. \parencite{Laerd2020}
	\item \textbf{Dichotomous} are nominal variables which have only two categories or levels. \parencite{Laerd2020}
    \item \textbf{Ordinal} are nominal variables that have two or more categories and are ordered or ranked. \parencite{Laerd2020}
\end{itemize}

The datasets to be examined in this thesis include continuous variables of the type interval, as well as all three types of categorical variables (see \ref{dataset_baysis} and \ref{dataset_arbis}).

\section{Coefficient Types}
\label{correlation_coefficient_types}
The datasets from \acrshort{baysis}, \acrshort{arbis} (see section \ref{dataset_baysis}, \ref{dataset_arbis}) as well as the generated data includes continuous, as well as categorical variables, describing interval, nominal, dichotomous and ordinal characteristics. For an exact analysis of relations between these characteristic the appropriate correlation coefficient suited for the apparent variables needs to be chosen. This task in itself is quite complex due to the number of coefficients to evaluate, amount of literature, numerous assumptions and the many disagreements in the field of statistics. From a comprehensive literature review the following correlation coefficients and tests, prominent in current studies and papers where selected, in dependency of their suitability for the apparent relation.

For a flattened introduction into correlation statistic, the article from Jun Ye \parencite{Yun2020} is highly recommended. % https://junye0798.com/post/everythin-you-need-to-know-about-correlation/

\subsection{Continuous - Continuous}
\label{correlation_pearson}

As stated in \ref{correlation_variable_types} continuous variables are metric measurements in the form or distances or durations. The most common correlation coefficient for continuous variables is the non-parametric Pearson's $r$.

\subsubsection{Pearson's $r$}

% https://statistikguru.de/spss/produkt-moment-korrelation/pearson-korrelation-in-spss.html

Pearson's $r$ describes the linear correlation of such continuous, non-ranked variables and does not assume normality or a normal distributed sample set as it is non-parametric. It is therefore suitable for the examination of continuous-continuous variable relations, where we can assume finite size of variance and covariance. \parencite{BenestyChenHuang2009,Sulthan2018}
 
The general correlation coefficient, shown in equation \ref{formula_correlation_basic} is the foundation for deducing Pearson's $r$. It is defined by the fraction of the covariance (see equation \ref{formula_correlation_covariant}) of two vectors $x$ / $y$ of length $i$ / $j$ and their standard deviation (see equation \ref{formula_correlation_deviation}).\parencite{HerzSchlicherSiegener1992}

\smallskip

\begin{equation}
\label{formula_correlation_basic}
	\rho = \frac{\sigma_{xy}}{\sigma_{x}\sigma_{y}}
\end{equation}

\begin{equation}
\label{formula_correlation_covariant}
	\sigma_{xy} = \sum_{ij}(x_i-\mu_X)(y_n-\mu_Y) \cdot p(x_i,y_j)
\end{equation}

\begin{equation}
\label{formula_correlation_deviation}
	\sigma_{x,y} = \sum_{i}(x_i,y_i-\mu_{x,y})^2 \cdot p_i
\end{equation}

\bigskip

The equation \ref{formula_pearson} shows Pearson's correlation coefficient $r$, which is a direct usage of the definition in \ref{formula_correlation_basic}, assuming that both data variables have the same length, named $i$. The symbols $\bar{x}$ and $\bar{y}$ correspond to the means of the data variable $x$ and $y$, respectively. \parencite{BenestyChenHuang2009,Zychlinski2018}

\smallskip

\begin{equation}
\label{formula_pearson}	
	r_{xy} =  \frac{\sum_{i}{(x_i-\bar{x})(y_i-\bar{y})}}{\sqrt{\sum_{i}{(x_i-\bar{x})^2}\sum_{i}{(y_i-\bar{y})^2}}}
\end{equation}

\bigskip

The equation can simplified to \ref{formula_pearson_simplified} with the $SS$ corresponding to the summed squares and $SP$ corresponding to summed products.

\smallskip

\begin{equation}
\label{formula_pearson_simplified}
	r =  \frac{SP_{xy}}{\sqrt{SS_x SS_y}}
\end{equation}

\subsubsection{Interpretation of $r$}
Pearson's $r$ can have values of the range $-1$ to $+1$. If one variable moves in the same direction as the other, it is called positive correlation, represented by a positive correlation coefficient. In the case of one variable moving in a positive direction, when a second variable is moving in a negative direction, the correlation is called negative and has a negative coefficient. Another characteristic is the ration of change in the variables. When both variables change at the same ratio, they are linearly correlated. When both variables do not change in the same ratio, then they are non-linearly or curvy-linear correlated. 

\subsubsection{Interpretation of effect size $|r|$}
The degree of correlation, also be described as strength of association, is also called effect size and shows how strong the two variables are related with each other. It is defined specifically for Person's $r$ as the absolute values of $r$, mathematical written as $|r|$. According to Cohen \parencite{Cohen1988} the effect sizes of each correlation coefficient falls into one of two categorizes $D$ and $R$. $D$ corresponds to coefficients utilizing the mean difference and standardized mean difference. He defined the values of $D$ coefficients as small $D$ = .20, medium $D$ = .50, and large $D$ = .80 \parencite{Piegorsch2002}. The group of $R$ coefficients includes measures based on variance \parencite{Walker2005}. Cohen propose vastly different values of .01, .06, and .14 serve as indicators of small, medium, and large effect sizes for the $R$ group \parencite{Cohen1988}. However, if these values fit the purpose of the analysis, depends on the underlying data and is at discretion of the researcher.

% http://www.leeds.ac.uk/educol/documents/00002182.htm
% https://www.psychometrica.de/effect_size.html
% https://www.cedu.niu.edu/~walker/personal/Walker%20Kendall's%20Tau.pdf

According to Cohen recommendations for mean-based coefficients \parencite{Cohen1988,Piegorsch2002,Walker2005} and with consideration of the guidelines of Wolfe \parencite{Wolfe2017} and Regber \parencite{Regber2016}, which induce induce a increased scale due to the high sensitivity of Pearson's $r$, the following rules are defined for the interpretation of the effect size of $r$.

\begin{itemize}
  \item When both variables change in the same ratio, the absolute value is 1.0, which is called perfect correlation.
  \item If the range is above .80, it is called high degree of correlation.
  \item A moderate degree of correlation lays in the range of .50 to .80.
  \item When range is between .30 to .50, it is called low degree of correlation.
  \item When it is lower than .30, it shows that there is no correlation, which can be called absence of correlation.
\end{itemize}	

There a many other effect sizes like Cohen's $w$, $f^2$ or Heghes $g$, which can be used for the interpretation of relations. But there is also a high amount of contradictions from one source to another, depending on the author, time of publication and data foundation. The interpretation of $r$ and its associated definition by Cohen is the most robust and common ground found in literature and will be used as mathematical and guideline base for other correlation coefficients.
% https://de.wikipedia.org/wiki/Effektst%C3%A4rke#Umrechnung_in_r

\subsubsection{Significance of $r$}
To determine if $r$, is statically significant, a chi-square test can be applied to find the $p$-value, testing the probability of independence. Equation \ref{formula_chi_squared_simplified} shows the chi-squared statistic taken from Wikipedia, cited from Karl Pearson \parencite{Pearson1990}.

\smallskip

%\begin{equation}
%\label{formula_chi_squared}	
%	\chi^2 = \sum_{i=1}^{m}{\frac{(N_i-n_{0i})^2}{n_{0i}}}
%\end{equation}

\begin{equation}
\label{formula_chi_squared_simplified}	
	\chi^2 = \sum_{i=1}^{n}{\frac{(O_i-E_i)^2}{E_i}} == N\sum_{i=1}^{n}{\frac{(O_i/N-p_i)^2}{p_i}}
\end{equation}

\begin{itemize}
	\setlength\itemsep{0.1em}	
	\item[] $N$ is the total number of data samples 
	\item[] $O_i$ is the number of data samples with type $i$
	\item[] $E_i = N p_i$ is the expected number of data samples with type $i$
\end{itemize}

The $p$-value can be comprised by comparing $\chi^2$ to a $\chi^2$-distribution by calculation or by using a conversion table \parencite{Piegorsch2002}, with the degree of freedom $df = (n_x - 1) \cdot (n_y - 1)$. The resulting  $p$-value, is compared to the significance level $\alpha$, to either accept ($p > \alpha$) or reject ($p <= \alpha$) the null hypothesis ("The means are equal to the population"). For a 2-tailed $p$-value, $p$ will be doubled to incorporate both ends of the distribution. Usually a value of $0.05$ is chosen for $\alpha$, which means that there is a 5\% risk of falsely rejecting the null hypothesis. From this definition the following two interpretations of the correlation coefficient can be drawn. \parencite{OTSD2020}

\begin{itemize}
	\item $p <= \alpha$ means that the null hypothesis can be rejected and indicates that is a significant dependency between the two tested variables. It can be concluded that the increase or decrease of one variable does significantly related to the increase or decrease of the other.
	\item $p > \alpha$ means that there is \textbf{no} significant dependency between the two variables and no conclusion can be drawn for the correlation.
\end{itemize}

\subsection{Continuous - Nominal}
This type of relation is objectively the most complex to evaluate. One well known method to analyze the relation between a continuous and categorical variable, which is not ranked and has more than to values, is the analysis of variance (ANOVA). As it is a parametric test it unfortunately assumes normal or gaussian distributed variables, which is not give in our datasets (see chapter \ref{data}). A non parametric approach of the ANOVA is the rather uncommon Kruskal-Wallis H-test \parencite{Leon1998}. Both tests indicates if at least one variable stochastically dominates another, but not in which groups or in how many groups this domination occurs \parencite{OTSD2020}. They therefore do not provide a statement about the correlation strength, but the statistical significance of variances between groups.

The research of a non-parametric correlation coefficient for the relation of a continuous and nominal variable only provided one suitable option, the eta ($\eta$) coefficient from Pearson \parencite{Benninghaus2007}, describing the relationship between variables based on the sums of squares used in the ANOVA \parencite{Lewis2012,Benninghaus2007}.

\subsubsection{Eta ($\eta$) coefficient}
% https://stackoverflow.com/questions/52083501/how-to-compute-correlation-ratio-or-eta-in-python/52084418
% https://www.frontiersin.org/articles/10.3389/fpsyg.2013.00863/full
The $\eta$ coefficient, also called correlation ratio, is a measurement for the proportion of the variation in $y$, which is associated with a membership different groups in $x$ \parencite{Laken2013}. Or mathematically, $\eta$ is the squared root of ratio between $SS_x$ and $SS_y$ \parencite{Shaldehi2013,SAGE2014}. When calculated the value of $\eta^2$ represents the percentage of total variance which can be accounted to a group relation.

\smallskip
\begin{equation}
\label{formula_eta}	
	\eta = \sqrt{\frac{SS_x}{SS_y}}
\end{equation}

 As the data doesn't fit the parametric requirements of ANOVA, the non-parametric Kruskal-Wallis H-test, shown in equation \ref{formula_kruskal_wallis}, will be used to test for variance and significance.

% https://de.wikipedia.org/wiki/Kruskal-Wallis-Test
% https://www.sciencedirect.com/topics/nursing-and-health-professions/kruskal-wallis-test
\smallskip
\begin{equation}
\label{formula_kruskal_wallis}	
	H = \frac{12}{n(n+1)}\sum_{h}{\frac{S_h^2}{n_h}}-3(n+1)
\end{equation}

\subsubsection{Interpretation of $\eta$}
The coefficient $\eta$ can have values in the range from $0$ to $1$ and equals to Person's $r$ according to \parencite{Laken2013}. The effect size, which is categorized in the $R$ group and can therefore be interpreted as follows \parencite{Regber2016,Cohen1988}.
% https://stats.stackexchange.com/questions/166696/how-do-i-convert-eta2-to-pearsons-r
% https://www.researchgate.net/post/Whats_the_reason_for_deriving_Cohens_w_from_Cramers_V_if_the_latter_is_already_a_measure_of_effect_size
\begin{itemize}
	\item $\eta <= .06$ : The correlation strength is weak
	\item $.06 < \eta < .14$ : A moderate strength of correlation
	\item $\eta >= .14$ : There is a strong correlation
\end{itemize}

For the Kruskal-Wallis H-test applies the following. The higher value of $H$, the higher the variance between the unique variables. The significance of $H$ must be tested with $\chi^2$.

\subsubsection{Significance of $\eta$}
The significancy evaluation for the $\eta$ coefficient is done via the $p$-value of the Kruskal-Wallis H-test, which is calculated with the chi-squared test ($\chi^2$), defined in sub section \textit{Significance of $r$} in \ref{correlation_pearson} \parencite{Filipiak2013}. In the case of both a correlated $\eta$ and a significant $H$, the exact groups, which are related need to be determined via a post-hoc test.

\subsection{Continuous - Dichotomous}
The Point Biserial correlation is a special form of the Pearson's $r$ correlation coefficient and suited to evaluate the association of continuous-dichotomous relations. 

\subsubsection{Point Biserial}
The Point Biserial notation, shown in formula \ref{formula_point_biserial}, can be derived from Person's $r$ with the assumption of $y$ only taking dichotomy values of 0 and 1, so that $\bar{y} = p$. The distinction of the cases

\begin{itemize}
	\item $i \cdot p$ referring to $y=1$ an with $1 - p = q$ bigger than $\bar{y}$
	\item $i \cdot q$ referring to $y=0$ an with $1 - p = -p$ smaller than $\bar{y}$
\end{itemize}
allow to form \ref{formula_point_biserial_from_pearson} from \ref{formula_pearson}, which can be simplified to \ref{formula_point_biserial}. \parencite{Tate1954,CohenWest2003,Bortz2004,DeJesus2019}

\smallskip

\begin{equation}
\label{formula_point_biserial_from_pearson}
	r_{pq} =  \frac{n \cdot p (\bar{x}_{y=1}-\bar{x}) \cdot q + n \cdot p (\bar{x}_{y=0}-\bar{x}) \cdot (-q)}{\sqrt{\sum_{i}{(x_i-\bar{x})^2} \cdot (n \cdot p \cdot q^2 + n \cdot q \cdot (-p)^2)}}
\end{equation}
%\begin{equation}
%\label{formula_point_biserial_from_pearson_simplyfied}
%	r_{pqi} =  \frac{n \cdot p \cdot q \cdot (\bar{x}_{y=1}-\bar{x}_{y=0})}{\sqrt{\sum_{i}{(x_i-\bar{x})^2} \cdot (n \cdot p \cdot q)}}
%\end{equation}
\begin{equation}
\label{formula_point_biserial}
	r_{pq} =  \frac{\bar{x}_{y=1}-\bar{x}_{y=0}}{\sqrt{\sum_{i}{(x_i-\bar{x})^2}}} \cdot \sqrt{n \cdot p \cdot q \cdot} 
\end{equation}

\smallskip

It must be pointed out that if the dichotomous variable is artificially binarized, i.e. there is likely continuous data underlying it, biserial correlation is a more a measurement of similarity instead of association.

\subsubsection{Interpretation of $r_{pq}$}
Due to the mathematical similarity of the Point Biserial to the Pearson's $r$, the general interpretation of Pearson's $r$ can be applied to Point Biserial with some adjustments. The range of the Point Biserial coefficient, from $0$ to $1$ removes the direction of correlation from the interpretation. According to Cohen \parencite{Cohen1988} the following can be used as guidelines for the effect size $r_{pq}$ \parencite{Leblanc2017}.

\begin{itemize}
	\item $r_{pq} <= \pm \: .10$ : The correlation strength is weak
	\item $\: .30 < r_{pq} < \: .50$ : A moderate strength of correlation
	\item $r_{pq} >= \: .50$ : There is a strong correlation
\end{itemize}

\subsubsection{Significance of $r_{pq}$}
The significancy evaluation of the Point Biserial coefficient is done via the 2-tailed $p$-value, a doubled chi-square test (see sub section \textit{Significance of $r$} in section \ref{correlation_pearson}).

\subsection{Continuous - Ordinal}
For ordinal variables, also called ranked or rank ordered, the commonly used Spearman's $\rho$ can be applied, but should be replaced by Kendall's $\tau$ because of it superiority over Spearman \parencite{Newson2002}. 

\subsubsection{Kendall's $\tau$}
Kendalls $\tau$ evaluates the order of rank pairs, instead of the squared rank difference, which make is more robust against outliners. Because we can assume that the data has ties implemented, the $\tau$ with ties must be used. The general definition is shown in equation \ref{formula_kendalls_r_complex}, with $P$ referring to the \textit{proversion} and $I$ to the \textit{inversion}. With the assumption that the continuous measurement $x$ is or can be ordered, the ordinal/ranked variable $y$ will be wrongly ordered. After forming all possible rank pairs between $x$ and $y$, $P$ and $I$ can be deduced and $\tau$ can be calculated. \parencite{Reiter2015,Bossart2017}

\begin{itemize}	
	\item[] \textbf{Proversion} ($+$) is the number if pairs, where $x < y$ 
	\item[] \textbf{Inversion} ($-$) is the number of pairs, where $x > y$ 
	\item[] \textbf{Ties} ($0$) are pairs where $x = y$
\end{itemize}

\begin{equation}
\label{formula_kendalls_r_complex}
	\tau = \frac{P-I}{\sqrt{(\frac{n(n-1)}{2}-T_x) \cdot (\frac{n(n-1)}{2}-T_y)}}
\end{equation}
\begin{equation}
\label{formula_kendalls_r_tx}
	T_x = \sum_{i=1}^n \frac{t_{x_i}(t_{x_i}-1)}{2}
\end{equation}
\begin{equation}
\label{formula_kendalls_r_ty}
	T_y = \sum_{j=1}^m \frac{t_{y_j}(t_{y_j}-1)}{2}
\end{equation}
\smallskip

\begin{itemize}
	\setlength\itemsep{0.1em}	
	\item[] $N$ is the total number of rank pairs 
	\item[] $P | I$ are the pro- and inversion of pairs
	\item[] $T_x | T_y$ are the ties in $x$ and $y$
	\item[] $n | m$ are the number of rank bindings in $x$ and $y$
	\item[] $t_{x_i} | t_{x_i}$ are the length of rank bindings in $x$ and $y$
\end{itemize}

\bigskip

Thought transformation we can simplify the general definition to equation \ref{formula_kendalls_r} \parencite{Reiter2015}.

\begin{equation}
\label{formula_kendalls_r}
	\tau = \frac{P-I}{\sqrt{(P+I+T_x) \cdot (P+I+T_y)}}
\end{equation}

\subsubsection{Interpretation of $\tau$}
% https://www.reddit.com/r/AskStatistics/comments/44ypc4/kendall_taus_effect_size_question/
Strictly speaking, Kendall's $\tau$ is not a measure of effect size, like Pearson's $r$, but tends to be of similar magnitude. Because of this similarity the general interpretation defined in the sub section \textit{Interpretation of effect size $|r|$} in \ref{correlation_pearson} can be applied. To adapt the guidelines to the lesser sensitivity of $\tau$, they are scaled downwards \parencite{Regber2016}.

\begin{itemize}
	\item $\tau <= \: .10$ : The correlation strength is weak
	\item $\: .30 < \tau < \: .50$ : A moderate strength of correlation
	\item $\tau >= \: .50$ : There is a strong correlation
\end{itemize}

\subsubsection{Significance}
The significancy evaluation of Kendall's $\tau$ coefficient is done via the 2-tailed $p$-value, elaborated in sub section \textit{Significance of $r$} in section \ref{correlation_pearson}.

\subsection{Categorical - Categorical}
The Pearson's $\chi^2$ test from section \ref{correlation_pearson} can also be applied to categorical data for independence statistics. Two correlation coefficients using $\chi^2$ are Cramer’s $V$ and Theil’s $U$. Both can be used to analyze categorical-categorical relations, but differ in the type of result they provide \parencite{OutsideTwoStandardDeviations2018}. Cramer’s $V$ is a symmetric measure, providing us with a measure of association strength. Theil’s $U$, the uncertainty coefficient, on the other hand is a conditional measure and represents the predictability of an association \parencite{Akoglu2018,StackExchange2020}. Because the Theil’s $U$ measurement of predictability provides a better interpretability, it is the preferred  result choice for the interpretation and implementation. 

\subsubsection{Cramer’s V}

Cramer’s V, also called Cramer's phi ($\Phi_c$), is a measurement for the relation of two nominal variables. In equation \ref{formula_cramers_v_biased}, showing the notation of Cramer’s $V$, $k$ and $r$ are the number of columns and row, respectively. $\varphi$, the phi coefficient, is defined by $\frac{{\chi^2}}{n_{ij}}$. The $\chi^2$ shown in \ref{formula_cramers_v_chi} is derived from equation \ref{formula_chi_squared_simplified} with the expansion to columns an rows. \parencite{Sheskin1997,Bergsma2013}
\smallskip
\begin{equation}
\label{formula_cramers_v_biased}
	V = \Phi_c =  \sqrt{\frac{{\varphi^2}}{min(k-1,r-1)}} = \sqrt{\frac{\frac{{\chi^2}}{n_{ij}}}{min(k-1,r-1)}}
\end{equation}
\begin{equation}
\label{formula_cramers_v_chi}
	\chi^2 =  \sum_{i,j}{\frac{(n_{ij}-\frac{n_i n_j}{n})^2}{\frac{n_i n_j}{n}}}
\end{equation}

\smallskip

The above notation of $\Phi_c$ can be heavily biased, trending to overestimate the strength of relation. It can be corrected with \ref{formula_cramers_v_corrected}, using the corrected notation \ref{formula_cramers_v_phi_corrected} for $\tilde{\varphi^2}$ and \ref{formula_cramers_v_k_corrected} as well as \ref{formula_cramers_v_k_corrected} for $k,r$. \parencite{Bergsma2013}
\smallskip
\begin{equation}
\label{formula_cramers_v_corrected}
	\tilde{V} = \tilde{\Phi_c} = \sqrt{\frac{\tilde{\varphi^2}}{min(\tilde{i_{max}}-1,\tilde{j_{max}}-1)}}
\end{equation}
\begin{equation}
\label{formula_cramers_v_phi_corrected}
	\tilde{\varphi^2} = max(0,\varphi^2 - \frac{(k-1)(r-1)}{n-1})
\end{equation}
\begin{equation}
\label{formula_cramers_v_k_corrected}
	\tilde{k} = k - \frac{(k-1)^2}{n-1}
\end{equation}
\begin{equation}
\label{formula_cramers_v_r_corrected}
	\tilde{r} = r - \frac{(r-1)^2}{n-1}
\end{equation}

\bigskip

\subsubsection{Theil’s U}
% https://rstudio-pubs-static.s3.amazonaws.com/558925_38b86f0530c9480fad4d029a4e4aea68.html
The uncertainty coefficient, also called entropy coefficient, is a measurement for the association between two nominal variables and in comparison to Cramers V, provides a much better predictability statement. It is based on the of concept of comparing the entropies of variables to determine a degree of association \parencite{Hoang2019}. The entropy of a distribution (see equation \ref{formula_theils_hx}) and the conditional entropy (see equation \ref{formula_theils_hxy}) are used to calculate the uncertainty coefficient $U(X)$ \parencite{Glen2017,Glen2018}, which tells us: given Y, what can fraction can be predicted for X \parencite{Hoang2019}.

\smallskip
\begin{equation}
\label{formula_theils}
	U(X) = \frac{H(X)-H(X|Y)}{H(X)}
\end{equation}
\begin{equation}
\label{formula_theils_hx}
	H(X) = -\sum_{x} p_{X}(x)log p_X(x)
\end{equation}
\begin{equation}
\label{formula_theils_hxy}
	H(X) = -\sum_{x,y} p_{X,Y}(x,y)log p_{X,Y}(x,y)
\end{equation}

% Definition: Theil’s U symmetrical
%\begin{equation}
%\label{formula_theils_sym}
%	U(X,Y) = \frac{H(X)U(X|Y)-H(Y)U(Y|X)}{H(X)+H()Y} = 2[\frac{H(X)+H(Y)-H(X|Y)}{H(X)+H(Y)}]
%\end{equation}

\subsubsection{Interpretation}
For the Interpretation of Cramers $V$ a adaption to Pearson's $r$ is necessary, but at the same time quite controversial. Some studies convert $V$ to the effect size $w$ for an equal measurement to $r$ ($r$ is representable over different studies), when $V$ is already a measure of effect size by itself. \parencite{Baguley2016}
%https://www.researchgate.net/post/Whats_the_reason_for_deriving_Cohens_w_from_Cramers_V_if_the_latter_is_already_a_measure_of_effect_size
% https://www.researchgate.net/post/How_can_I_intepret_the_effect_sizes_of_Cramers_V_when_DF_3

\smallskip
\begin{equation}
\label{formula_cohens_w}
	w = V \cdot \sqrt{min(i_{max}-1,j_{max}-1)}
\end{equation}

\medskip

As shown in equation \ref{formula_cohens_w} \parencite{Baguley2016}, the conversion from $V$ to $w$ is similar to the reduction of $\phi$ in the definition of $V$ (see equation \ref{formula_cramers_v_corrected}). This advocates the opinion that the conversion to $w$ is necessary within a single study and a adaption of scale is sufficient \parencite{Baguley2016}. For this thesis the decision was made to used an adapted scale of effect size according to Ellis for interpreting $\eta$ in the range of $0$ to $1$ \parencite{Cohen1988,Ellis2010,Hemmerich2019}.

\begin{itemize}
	\item $V <= \: .10$ : The correlation strength is weak
	\item $\: .30 < V < \: .50$ : A moderate strength of correlation
	\item $V >= \: .50$ : There is a strong correlation
\end{itemize}

The values of Theil's $U$ shows the predictability of the association, where the following rules for interpretation. \parencite{TheilsInt01,TheilsInt02,TheilsInt03}

\begin{itemize}
	\item $U < \: 1$ : The forecasting is better than guessing.
	\item $U \sim \: 1$ : The forecasting is about as good as guessing.
	\item $U > \: 1$ : The forecasting is worse than guessing.
\end{itemize}

\section{Correlation Matrix}
As a result we have the following correlation coefficients and statistical test to be used for the mixed analysis of continuous and categorical variables. These will be implemented into a processing script in section \autoref{methodology_correlation_processing}.

\bigskip

\begin{table}[ht]
	\centering
    \begin{tabular}{c|c|c|c|c}
        \toprule
								& \textbf{Ordinal} 	& \textbf{Dichotomous} 	&  \textbf{Nominal}	& \textbf{Continuous}	\\
		\midrule
		\textbf{Ordinal}		& Cramer’s $V$		& Cramer’s $V$			& Cramer’s $V$		& Kendall's $\tau$		\\
		\midrule
		\textbf{Dichotomous}	& Cramer’s $V$		& Cramer’s $V$			& Cramer’s $V$		& Point Biserial $r$	\\
		\midrule
		\textbf{Nominal}		& Cramer’s $V$		& Cramer’s $V$			& Cramer’s $V$		& Eta $\eta$			\\
		\midrule
		\textbf{Continuous}		& Kendall's $\tau$ 	& Point Biserial $r$	& Eta $\eta$		& Pearson's $r$			\\
		\bottomrule
	\end{tabular}
	\caption{Correlation Coefficients Matrix}
	\label{tab:correlation_coefficient_matrix}
\end{table}

\subsubsection{Significance vs. Uncertainty}
\label{correlation_significance_uncertainty}

\todo{Define the usage of significance vs uncertainty in general}

\todo{Define the usage of significance vs uncertainty for Cramer's $V$ and Theil's $U$}

%https://de.statista.com/statistik/lexikon/definition/122/signifikanz/
%https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3387884/
%https://www.ons.gov.uk/methodology/methodologytopicsandstatisticalconcepts/uncertaintyandhowwemeasureit
%https://www.ncbi.nlm.nih.gov/books/NBK459346/
%https://www.npr.org/sections/health-shots/2019/03/20/705191851/statisticians-call-to-arms-reject-significance-and-embrace-uncertainty?t=1600534742009
%https://www.tandfonline.com/doi/full/10.1080/00223131.2013.820155

\section{Significance Specification}

\todo{Explain the process of why and how the correlation results are tested and further evaluated}

% https://de.wikipedia.org/wiki/Wilcoxon-Mann-Whitney-Test
% https://pythonfordatascienceorg.wordpress.com/wilcoxon-sign-ranked-test-python/
%The Wilcoxon signed-rank test is the non-parametric univariate test which is an alternative to the dependent t-test. It also is called the Wilcoxon T test, most commonly so when the statistic value is reported as a T value. Which scipy.stats.wilcoxon() uses for it’s calculation. This is the recommended test to use when the data violates the assumption of normality. It’s used to test if there is a significant difference on scores when there is a “before” and “after” condition of some treatment or intervention. An example of this is if you where to collect the blood pressure for an individual before and after some treatment, condition, or time point.
%
%The hypothesis being test is:
%
%Null hypothesis (H0): The difference between the pairs follows a symmetric distribution around zero.
%Alternative hypothesis (HA): The difference between the pairs does not follow a symmetric distribution around zero.
%If the p-value is less than what is tested at, most commonly 0.05, one can reject the null hypothesis.
